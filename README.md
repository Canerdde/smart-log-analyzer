# ğŸ” Smart Log Analyzer

Intelligent log analysis and monitoring system - A production-ready log analysis platform built with Python and FastAPI.

## ğŸ“‹ Features

### Core Features
- âœ… **Log File Upload**: Multiple file upload with drag & drop support
- ğŸ” **Automatic Analysis**: Logs are automatically parsed and analyzed
- ğŸ“Š **Classification**: Classification by ERROR, WARNING, INFO, DEBUG levels
- ğŸ”¥ **Error Detection**: Detects frequently recurring errors
- ğŸ“ˆ **Visualization**: Analysis results with charts and statistics
- ğŸ¤– **AI Integration**: Log comments and recommendations with OpenAI (optional)
- ğŸ’¾ **Database**: Data storage with PostgreSQL
- ğŸ³ **Docker**: Easy setup with Docker and Docker Compose
- ğŸ“š **API Documentation**: Interactive API documentation with Swagger UI

### Advanced Features
- ğŸ“¡ **Real-time Streaming**: Live log monitoring via WebSocket
- ğŸ”– **Saved Searches**: Save frequently used searches
- â­ **Favorites**: Add important log files to favorites
- ğŸš¨ **Alert System**: Email/Slack/webhook notifications for critical errors
- ğŸ’¬ **Log Comments**: Add notes to log lines
- âŒ¨ï¸ **Keyboard Shortcuts**: Shortcuts for quick access
- ğŸ“Š **Log Comparison**: Compare two log files
- ğŸ” **Advanced Filtering**: Regex, date range, multi-condition filtering
- ğŸ·ï¸ **Tags and Categories**: Add tags and categories to files
- ğŸ“¦ **Bulk Operations**: Multiple file selection and bulk operations
- ğŸ“¥ **Export**: Export in PDF, Excel, JSON, XML formats
- ğŸ“œ **Search History**: Save recent searches and quick access
- ğŸ“Š **Dashboard Widgets**: Customizable dashboard
- ğŸ¨ **Log Coloring**: Syntax highlighting and log level-based coloring
- ğŸ” **Pattern Detection**: Automatic pattern detection and grouping
- ğŸ“… **Timeline View**: Log visualization on timeline
- ğŸ‘¥ **Multi-user Support**: User accounts and role-based access control
- ğŸ“Š **Log Aggregation**: Collect logs from multiple sources
- ğŸ¤– **ML Anomaly Detection**: Anomaly detection with Machine Learning
- ğŸ”— **Log Correlation**: Relationship analysis between log files
- âš¡ **Performance Metrics**: Response time, throughput analysis
- ğŸ”Œ **Integrations**: Slack, Teams, Jira, Trello integrations
- ğŸ“± **Responsive Design**: Mobile and tablet compatible design

## ğŸš€ Quick Start

### Running with Docker (Recommended)

1. Clone the repository:
```bash
git clone <repo-url>
cd smart-log-analyzer
```

2. Start with Docker Compose:
```bash
docker-compose up -d
```

3. Access the application:
- Web Interface: http://localhost:8000
- API Documentation: http://localhost:8000/api/docs

### Manual Installation

1. Install requirements:
```bash
pip install -r requirements.txt
```

2. Create PostgreSQL database:
```sql
CREATE DATABASE loganalyzer;
```

3. Create `.env` file:
```bash
cp env.example .env
# Edit the .env file
```

4. Run the application:
```bash
uvicorn app.main:app --reload
```

## ğŸ“ Project Structure

```
smart-log-analyzer/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py              # FastAPI application main file
â”‚   â”œâ”€â”€ database.py          # Database configuration
â”‚   â”œâ”€â”€ models.py            # SQLAlchemy models (User, LogFile, Tag, Category, etc.)
â”‚   â”œâ”€â”€ schemas.py           # Pydantic schemas
â”‚   â”œâ”€â”€ auth.py              # JWT authentication utilities
â”‚   â”œâ”€â”€ log_parser.py        # Log parsing module
â”‚   â”œâ”€â”€ analyzer.py          # Log analysis module
â”‚   â”œâ”€â”€ ai_service.py        # AI integration (optional)
â”‚   â”œâ”€â”€ pattern_detection.py # Pattern detection module
â”‚   â”œâ”€â”€ export.py            # PDF/Excel export module
â”‚   â”œâ”€â”€ integrations.py      # Slack/Teams/Jira/Trello integrations
â”‚   â”œâ”€â”€ cache.py             # Redis cache (optional)
â”‚   â”œâ”€â”€ tasks.py             # Celery background tasks (optional)
â”‚   â”œâ”€â”€ monitoring.py        # Prometheus metrics (optional)
â”‚   â”œâ”€â”€ ml/                  # Machine Learning modules
â”‚   â”‚   â””â”€â”€ anomaly_detection.py
â”‚   â””â”€â”€ api/                 # API endpoints
â”‚       â”œâ”€â”€ auth.py          # Authentication endpoints
â”‚       â”œâ”€â”€ logs.py          # Log upload endpoints
â”‚       â”œâ”€â”€ analysis.py      # Analysis endpoints
â”‚       â”œâ”€â”€ dashboard.py     # Dashboard endpoints
â”‚       â”œâ”€â”€ stream.py        # WebSocket streaming
â”‚       â”œâ”€â”€ alerts.py        # Alert management
â”‚       â”œâ”€â”€ tags.py          # Tag and category management
â”‚       â”œâ”€â”€ favorites.py     # Favorites management
â”‚       â”œâ”€â”€ saved_searches.py # Saved searches
â”‚       â”œâ”€â”€ search_history.py # Search history
â”‚       â”œâ”€â”€ comments.py      # Log entry comments
â”‚       â”œâ”€â”€ comparison.py    # Log comparison
â”‚       â”œâ”€â”€ export.py        # Export endpoints
â”‚       â”œâ”€â”€ aggregation.py   # Log aggregation
â”‚       â”œâ”€â”€ correlation.py   # Log correlation
â”‚       â”œâ”€â”€ performance.py   # Performance metrics
â”‚       â”œâ”€â”€ integrations.py  # Integration endpoints
â”‚       â””â”€â”€ ml.py            # ML endpoints
â”œâ”€â”€ static/
â”‚   â””â”€â”€ index.html           # Web interface (single-page application)
â”œâ”€â”€ mobile/                  # React Native mobile app (optional)
â”œâ”€â”€ uploads/                 # Uploaded log files
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ Dockerfile              # Docker image definition
â”œâ”€â”€ docker-compose.yml      # Docker Compose configuration
â”œâ”€â”€ env.example             # Environment variables example
â””â”€â”€ README.md
```

## ğŸ”Œ API Endpoints

### Authentication
- `POST /api/auth/register` - User registration
- `POST /api/auth/login` - User login
- `GET /api/auth/me` - Current user information
- `PUT /api/auth/profile` - Profile update

### Log Files
- `POST /api/logs/upload` - Upload log file (multiple file support)
- `GET /api/logs/` - List all log files (filtering, tags, categories)
- `GET /api/logs/{file_id}` - Get specific log file
- `DELETE /api/logs/{file_id}` - Delete log file
- `POST /api/logs/bulk-delete` - Bulk file deletion
- `POST /api/logs/bulk-export` - Bulk export

### Analysis
- `GET /api/analysis/{file_id}` - Get analysis results
- `GET /api/analysis/{file_id}/entries` - Get log entries
- `GET /api/analysis/{file_id}/errors` - Get only errors
- `GET /api/analysis/{file_id}/warnings` - Get only warnings
- `GET /api/analysis/{file_id}/patterns` - Pattern detection results
- `GET /api/analysis/{file_id}/timeline` - Timeline view

### Dashboard
- `GET /api/dashboard/stats` - General statistics

### Real-time Streaming
- `WS /api/ws/logs/{file_id}` - Live log streaming via WebSocket

### Alerts
- `GET /api/alerts/` - List alert rules
- `POST /api/alerts/` - Create new alert rule
- `DELETE /api/alerts/{alert_id}` - Delete alert rule

### Tags & Categories
- `GET /api/tags/` - List all tags
- `POST /api/tags/` - Create new tag
- `GET /api/categories/` - List all categories
- `POST /api/categories/` - Create new category

### Favorites & Saved Searches
- `GET /api/favorites/` - List favorite files
- `POST /api/favorites/` - Add to favorites
- `DELETE /api/favorites/{file_id}` - Remove from favorites
- `GET /api/saved-searches/` - List saved searches
- `POST /api/saved-searches/` - Save new search

### Export
- `GET /api/export/{file_id}/pdf` - PDF export
- `GET /api/export/{file_id}/excel` - Excel export
- `GET /api/export/{file_id}/json` - JSON export
- `GET /api/export/{file_id}/xml` - XML export

### ML & Analytics
- `GET /api/ml/{file_id}/anomalies` - Anomaly detection
- `GET /api/correlation/event-chain` - Log correlation
- `GET /api/performance/response-times` - Performance metrics
- `GET /api/aggregation/combined-logs` - Log aggregation

For detailed documentation of all endpoints: http://localhost:8000/api/docs

## ğŸ› ï¸ Technologies

### Backend
- **Framework**: FastAPI, Python 3.11+
- **Database**: PostgreSQL
- **ORM**: SQLAlchemy
- **Authentication**: JWT (python-jose, bcrypt)
- **Background Jobs**: Celery (optional)
- **Caching**: Redis (optional)
- **ML/AI**: scikit-learn, OpenAI API (optional)
- **Export**: ReportLab (PDF), openpyxl (Excel)
- **Monitoring**: Prometheus (optional)

### Frontend
- **Framework**: Vanilla JavaScript
- **Charts**: Chart.js
- **Styling**: CSS Variables (Dark/Light Mode)
- **Real-time**: WebSocket

### Infrastructure
- **Containerization**: Docker, Docker Compose
- **CI/CD**: GitHub Actions (optional)
- **Mobile**: React Native, Expo (optional)

## ğŸ“Š Log Format Support

The system supports the following log formats:
- ISO 8601 date formats (2024-01-15 10:30:45)
- Various log level formats (ERROR, WARNING, INFO, DEBUG)
- Customizable parsing rules

## ğŸ¤– AI Integration

To use AI analysis:
1. Add your OpenAI API key to the `.env` file:
```
OPENAI_API_KEY=your_api_key_here
```

2. Check the "Analyze with AI" option when uploading logs.

## ğŸ”§ Configuration

To create a `.env` file, copy the `env.example` file:
```bash
cp env.example .env
```

Required environment variables:
- `DATABASE_URL`: PostgreSQL connection string (e.g., `postgresql://user:password@localhost:5432/loganalyzer`)
- `OPENAI_API_KEY`: OpenAI API key (optional, for AI features)
- `SECRET_KEY`: Secret key for JWT tokens (must be changed in production)
- `REDIS_URL`: Redis connection URL (optional, for caching)
- `CELERY_BROKER_URL`: Celery broker URL (optional, for background jobs)

**âš ï¸ Important**: The `.env` file is in `.gitignore` and will not be pushed to GitHub. Keep your API keys secure!

## ğŸ” Security

- `.env` file is in `.gitignore` and will not be committed
- Authentication with JWT tokens
- Password hashing with bcrypt
- Role-based access control (admin, user, viewer)
- API endpoints require authentication (except optional endpoints)

## ğŸ“ License

This project is for example and educational purposes.

## ğŸ‘¨â€ğŸ’» Developer Notes

- Log parsing algorithms can be customized in `app/log_parser.py`
- New analysis metrics can be added to `app/analyzer.py`
- API endpoints are in modular structure in `app/api/` folder

## ğŸ› Troubleshooting

**Database connection error:**
- If using Docker Compose, start all services with `docker-compose up -d`
- In manual installation, ensure PostgreSQL is running

**File upload error:**
- Ensure the `uploads/` directory has write permissions

**AI analysis not working:**
- Check that your OpenAI API key is valid
- The system works without API key, only AI features will be disabled
